{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import skimage.io\n",
    "import skimage.color as color\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "np.random.seed(123)\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "import keras\n",
    "from keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "from keras import backend as K\n",
    "import itertools\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the JPEGs and Segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all jpeg images stored in \"Data/Images\" to a list called im_lst\n",
    "\n",
    "im_dir = './ISIC-Archive-Downloader/Data/Images'\n",
    "\n",
    "im_glob = os.path.join(im_dir, '*.jpeg')\n",
    "im_lst = glob.glob(im_glob)\n",
    "\n",
    "# Importing all png images stored in \"Data/Segmentation\" to a list called im_lst_seg\n",
    "\n",
    "im_dir_seg = './ISIC-Archive-Downloader/Data/Segmentation'\n",
    "\n",
    "im_glob_seg = os.path.join(im_dir_seg, '*.png')\n",
    "im_lst_seg = glob.glob(im_glob_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all json images stored in \"Data/Descriptions\" to a list called json_lst\n",
    "\n",
    "json_dir = './ISIC-Archive-Downloader/Data/Descriptions'\n",
    "\n",
    "json_glob = os.path.join(json_dir,'*')\n",
    "json_lst = glob.glob(json_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame of the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "for file in json_lst:\n",
    "    with open(file, 'r') as fp:\n",
    "        obj = json.load(fp)\n",
    "        meta_dict = obj['meta']['clinical']\n",
    "        meta_dict['name'] = obj['name']\n",
    "        meta_dict['path'] = im_dir + '/{}.jpeg'.format(meta_dict['name']) \n",
    "        df_add = pd.DataFrame(meta_dict, index=[0])\n",
    "    df_final = df_final.append(df_add)\n",
    "\n",
    "df_final[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the dataframe WARNING: LONG TIME TO RUN\n",
    "\n",
    "# Turning age to floats and replacing NaN's with mean\n",
    "df_final['age_approx'] = pd.to_numeric(df_final['age_approx'], errors='coerce')\n",
    "df_final['age_approx'].fillna((df_final['age_approx'].mean()), inplace=True)\n",
    "\n",
    "# Turning benign_malignant columns to 0's and 1's and storing it in a column called \"result\"\n",
    "df_final['result'] = (df_final.benign_malignant == \"malignant\").astype('int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df_final['path'].map(lambda x: np.asarray(Image.open(x).resize((100,100))))\n",
    "#df_final\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating target and features (we need this to create the testing data)\n",
    "# Target = y = result col\n",
    "# Features = rest of df_final\n",
    "\n",
    "target = df_final['result']\n",
    "features=df_final.drop(columns=['benign_malignant', 'result'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training and Test Data\n",
    "Training:Test Split is 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(images, target, test_size=0.20,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the image data \n",
    "x_train = np.asarray(x_train_o.tolist())\n",
    "x_test = np.asarray(x_test_o.tolist())\n",
    "\n",
    "# Normalize training and test x data\n",
    "x_train_mean = np.mean(x_train)\n",
    "x_train_std = np.std(x_train)\n",
    "\n",
    "x_test_mean = np.mean(x_test)\n",
    "x_test_std = np.std(x_test)\n",
    "\n",
    "x_train = (x_train - x_train_mean)/x_train_std\n",
    "x_test = (x_test - x_test_mean)/x_test_std\n",
    "\n",
    "# Creating one hot encoded y_train and y_test\n",
    "y_train = to_categorical(y_train_o, num_classes = 2)\n",
    "y_test = to_categorical(y_test_o, num_classes = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.1, random_state = 2)\n",
    "x_train = x_train.reshape(x_train.shape[0], *(100, 100, 3))\n",
    "x_test = x_test.reshape(x_test.shape[0], *(100, 100, 3))\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *(100, 100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 100, 3)\n",
    "num_classes = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape, dilation_rate=2))\n",
    "model.add(Conv2D(32,kernel_size=(3, 3), activation='relu',padding = 'Same',stries=2))\n",
    "model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.40))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "epochs = 50 \n",
    "batch_size = 10\n",
    "history = model.fit_generator(x_train, y_train,\n",
    "                              epochs = epochs, validation_data = (x_validate,y_validate),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all jpeg images stored in \"Data/Images\" to a list called im_lst\n",
    "\n",
    "im_dir = './ISIC-Archive-Downloader/Data/Images'\n",
    "\n",
    "im_glob = os.path.join(im_dir, '*.jpeg')\n",
    "im_lst = glob.glob(im_glob)\n",
    "\n",
    "# Importing all png images stored in \"Data/Segmentation\" to a list called im_lst_seg\n",
    "\n",
    "im_dir_seg = './ISIC-Archive-Downloader/Data/Segmentation'\n",
    "\n",
    "im_glob_seg = os.path.join(im_dir_seg, '*.png')\n",
    "im_lst_seg = glob.glob(im_glob_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all json images stored in \"Data/Descriptions\" to a list called json_lst\n",
    "\n",
    "json_dir = './ISIC-Archive-Downloader/Data/Descriptions'\n",
    "\n",
    "json_glob = os.path.join(json_dir,'*')\n",
    "json_lst = glob.glob(json_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame of the Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "for file in json_lst:\n",
    "    with open(file, 'r') as fp:\n",
    "        obj = json.load(fp)\n",
    "        meta_dict = obj['meta']['clinical']\n",
    "        meta_dict['name'] = obj['name']\n",
    "        meta_dict['path'] = im_dir + '/{}.jpeg'.format(meta_dict['name']) \n",
    "        df_add = pd.DataFrame(meta_dict, index=[0])\n",
    "    df_final = df_final.append(df_add)\n",
    "\n",
    "df_final[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the dataframe WARNING: LONG TIME TO RUN\n",
    "\n",
    "# Turning age to floats and replacing NaN's with mean\n",
    "df_final['age_approx'] = pd.to_numeric(df_final['age_approx'], errors='coerce')\n",
    "df_final['age_approx'].fillna((df_final['age_approx'].mean()), inplace=True)\n",
    "\n",
    "# Turning benign_malignant columns to 0's and 1's and storing it in a column called \"result\"\n",
    "df_final['result'] = (df_final.benign_malignant == \"malignant\").astype('int')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df_final['path'].map(lambda x: np.asarray(Image.open(x).resize((100,100))))\n",
    "#df_final\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating target and features (we need this to create the testing data)\n",
    "# Target = y = result col\n",
    "# Features = rest of df_final\n",
    "\n",
    "target = df_final['result']\n",
    "features=df_final.drop(columns=['benign_malignant', 'result'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training and Test Data\n",
    "Training:Test Split is 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(images, target, test_size=0.20,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the image data \n",
    "x_train = np.asarray(x_train_o.tolist())\n",
    "x_test = np.asarray(x_test_o.tolist())\n",
    "\n",
    "# Normalize training and test x data\n",
    "x_train_mean = np.mean(x_train)\n",
    "x_train_std = np.std(x_train)\n",
    "\n",
    "x_test_mean = np.mean(x_test)\n",
    "x_test_std = np.std(x_test)\n",
    "\n",
    "x_train = (x_train - x_train_mean)/x_train_std\n",
    "x_test = (x_test - x_test_mean)/x_test_std\n",
    "\n",
    "# Creating one hot encoded y_train and y_test\n",
    "y_train = to_categorical(y_train_o, num_classes = 2)\n",
    "y_test = to_categorical(y_test_o, num_classes = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.1, random_state = 2)\n",
    "x_train = x_train.reshape(x_train.shape[0], *(100, 100, 3))\n",
    "x_test = x_test.reshape(x_test.shape[0], *(100, 100, 3))\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *(100, 100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 100, 3)\n",
    "num_classes = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape, dilation_rate=2))\n",
    "model.add(Conv2D(32,kernel_size=(3, 3), activation='relu',padding = 'Same',strides=2))\n",
    "model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.40))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "epochs = 50 \n",
    "batch_size = 10\n",
    "history = model.fit_generator(x_train, y_train,\n",
    "                              epochs = epochs, validation_data = (x_validate,y_validate),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all jpeg images stored in \"Data/Images\" to a list called im_lst\n",
    "\n",
    "im_dir = './Downloader/Data/sm100x100images'\n",
    "\n",
    "im_glob = os.path.join(im_dir, '*.jpeg')\n",
    "im_lst = glob.glob(im_glob)\n",
    "\n",
    "json_dir = './Downloader/Data/Descriptions'\n",
    "\n",
    "json_glob = os.path.join(json_dir,'*')\n",
    "json_lst = glob.glob(json_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "for file in json_lst:\n",
    "    with open(file, 'r') as fp:\n",
    "        obj = json.load(fp)\n",
    "        meta_dict = obj['meta']['clinical']\n",
    "        meta_dict['name'] = obj['name']\n",
    "        meta_dict['path'] = im_dir + '/{}.jpeg'.format(meta_dict['name']) \n",
    "        df_add = pd.DataFrame(meta_dict, index=[0])\n",
    "    df_final = df_final.append(df_add)\n",
    "\n",
    "df_final[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splice_benign = pd.values_count(df_final.benign_malignant.values, sort = False)['malignant']\n",
    "df_benign = df_final.sort(\"benign_malignant\")[:splice_benign]\n",
    "\n",
    "df_malignant= df_final[df_final.benign_malignant=='malignant']\n",
    "\n",
    "images_benign = df_benign['path'].map(lambda x: np.asarray(Image.open(x)))\n",
    "images_malignant = df_malignant['path'].map(lambda x: np.asarray(Image.open(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images_benign + images_malignant\n",
    "target = np.append(np.ones(len(images_benign)), np.zeros(len(images_benign)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(images, target, test_size=0.20,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the image data \n",
    "x_train = np.asarray(x_train_o.tolist())\n",
    "x_test = np.asarray(x_test_o.tolist())\n",
    "\n",
    "# Normalize training and test x data\n",
    "x_train_mean = np.mean(x_train)\n",
    "x_train_std = np.std(x_train)\n",
    "\n",
    "x_test_mean = np.mean(x_test)\n",
    "x_test_std = np.std(x_test)\n",
    "\n",
    "x_train = (x_train - x_train_mean)/x_train_std\n",
    "x_test = (x_test - x_test_mean)/x_test_std\n",
    "\n",
    "# Creating one hot encoded y_train and y_test\n",
    "y_train = to_categorical(y_train_o, num_classes = 2)\n",
    "y_test = to_categorical(y_test_o, num_classes = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.1, random_state = 2)\n",
    "x_train = x_train.reshape(x_train.shape[0], *(100, 100, 3))\n",
    "x_test = x_test.reshape(x_test.shape[0], *(100, 100, 3))\n",
    "x_validate = x_validate.reshape(x_validate.shape[0], *(100, 100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (100, 100, 3)\n",
    "num_classes = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape, dilation_rate=2))\n",
    "model.add(Conv2D(32,kernel_size=(3, 3), activation='relu',padding = 'Same',strides=2))\n",
    "model.add(MaxPool2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.40))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
